<html>
<head>
    <meta charset="utf-8"/>
<meta name="description" content=""/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>制作“漫画网站”爬虫 | buzi</title>
<link rel="shortcut icon" href="https://littlebuzi.github.io//favicon.ico?v=1591632652723">
<link href="https://cdn.bootcss.com/font-awesome/5.11.2/css/all.css" rel="stylesheet">
<link rel="stylesheet" href="https://littlebuzi.github.io//styles/main.css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
      integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

<script src="https://cdn.bootcss.com/highlight.js/9.15.10/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dockerfile.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dart.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/go.min.js"></script>
<script src="https://cdn.bootcss.com/moment.js/2.23.0/moment.min.js"></script>
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
        integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
        integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
        crossorigin="anonymous"></script>

<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->



    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <nav class="navbar navbar-expand-lg">
    <div class="navbar-brand">
        <img class="user-avatar" src="/images/avatar.png" alt="头像">
        <div class="site-name gt-c-content-color-first">
            buzi
        </div>
    </div>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
                <div class="nav-item">
                    
                        <a href="/" class="menu gt-a-link">
                            首页
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/archives" class="menu gt-a-link">
                            时间线
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/tags" class="menu gt-a-link">
                            分类
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="https://littlebuzi.github.io/about" class="menu gt-a-link">
                            关于
                        </a>
                    
                </div>
            
        </div>
    </div>
</nav>
    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    制作“漫画网站”爬虫
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        · 2019-12-01 ·
                    </time>
                    
                </div>
                <div class="post-content">
                    <h1 id="v10">v1.0</h1>
<h2 id="目的">目的</h2>
<p>1.目标网站：https://www.这里是网址.cc/</p>
<figure data-type="image" tabindex="1"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565345268608.png" alt="" loading="lazy"></figure>
<p>2.目标结果：获取全部漫画图片文件，并分好文件夹</p>
<hr>
<br>
<h2 id="实现过程">实现过程</h2>
<br>
<h2 id="基本逻辑">基本逻辑</h2>
<figure data-type="image" tabindex="2"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565360279264.png" alt="" loading="lazy"></figure>
<h2 id="代码实现">代码实现</h2>
<br>
<pre><code>
import requests
from bs4 import BeautifulSoup
import re
import os

#1-1030
for num1 in range(1,1031):
    circle = requests.get('https://这里是网址/book/'+str(num1))
    # 将获取的图片地址依次放入count中
    count = []
    # 将获取的网页内容放入BeautifulSoup
    soup = BeautifulSoup(circle.text, 'lxml')
    # 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.find_all('a'):
        if(book_name.string!='韩国'and book_name.string!='男性'):
            book_name_clean=book_name.string
            print(num1, book_name_clean)

os.makedirs('D://manhua//整站漫画爬取//' + str(num1) +'.'+ book_name_clean )

#menu_path_num = []

for item in soup.select('.d_menu&gt;ul&gt;li'):
    # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
    for a in item.find_all('a'):
        #print('a', a)
        # m 是 img标签中存在的属性
        menu_path = 'https://www.manhwa.cc/' + a.get('href')
        #count.append(menu_path)
        #menu_path_num.append(re.findall(r&quot;\d+\.?\d*&quot;, menu_path))
        menu_path_num=re.findall(r&quot;\d+\.?\d*&quot;, menu_path)

        #当前一部书爬取循环，从上面得到每一章地址后，遍历这么多“章”次

        #for num in menu_path_num:
        print('book_url:',menu_path)
        circle = requests.get(menu_path)
        # 将获取的图片地址依次放入count中
        count = []
        # 将获取的网页内容放入BeautifulSoup
        soup = BeautifulSoup(circle.text, 'lxml')
        # 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list
				
        for title in soup.select('div.fl.r_tab_l'):
            for title in title.find_all('span'):
                print('title:', title.text)
                title=title.text

        for item in soup.select('.r_img'):
            # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
            for img in item.find_all('img'):
                print('img_url:', img)
                # m 是 img标签中存在的属性
                img_path = img.get('data-original')
                count.append(img_path)
        # 用enumerate依次取出count中的图片地址 放入v中
        os.makedirs('D://manhua//整站漫画爬取//' +  book_name_clean + '//' + str(title) + '//')
        for i, v in enumerate(count):
            # 将获取的v值再次放入request中进行与网站相应
            image = requests.get(v)
            # 存取图片过程中，出现不能存储 int 类型，故而，我们对他进行类型转换 str()。w:读写方式打开，b：二进制进行读写。图片一般用到的都是二进制。
            with open('D://manhua//整站漫画爬取//' + book_name_clean + '//'+ str(title) + '//' +str(i) + '.jpg', 'wb') as file:
            #with open('C://Users//50159//Desktop//manhua//test//' + str(num1) + '_' + str(i) + '.jpg', 'wb') as file:
                # content：图片转换成二进制，进行保存。
                file.write(image.content)
            print(i)
</code></pre>
<p>到这基本工作已完成，进入测试阶段，出现以下</p>
<hr>
<br>
<h2 id="测试问题">测试问题</h2>
<p>1.第250本左右，书名字开始出现异常，爬取书名有其他文字并出现混乱，因为之前是通过最前面几本书的情况，通过抛弃字样，来筛选出书名，而后1030本里标签发生变动，所以之后通过只取第一个出现的标签代替现在的筛选。</p>
<p>2.文件夹命名及生成文件夹出错，由于整理时出现混乱，代码写重复了。而后修改。</p>
<p>3.中途停止，可能是网站识别到了这是爬虫，而后添加伪浏览器头部head，还是会停，基本是connect超时。</p>
<p>针对上面问题，修改成了2.0版本:</p>
<pre><code>
import requests
from bs4 import BeautifulSoup
import re
import os

#1-1030
for num1 in range(2,1031):
    #716字符问题无法生成文件夹
import urllib.request  # url包

def openUrl(circle):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36',
        'Host': 'jandan.net'
    }
    req = urllib.request.Request(circle, headers=headers)
    response = urllib.request.urlopen(req)  # 请求
    html = response.read()  # 获取
    html = html.decode(&quot;utf-8&quot;)  # 解码
    print(html)  # 打印

if __name__ == &quot;__main__&quot;:
    circle = requests.get('https://这里是网址/book/' + str(num1))

# 将获取的图片地址依次放入count中
count = []
# 将获取的网页内容放入BeautifulSoup
soup = BeautifulSoup(circle.text, 'lxml')
# 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.select('a')[0]:
        book_name_clean = book_name.string
        print(num1, book_name_clean)

#os.makedirs('D://manhua//整站漫画爬取//' + str(num1) +'.'+ book_name_clean )

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.find_all('a'):
        if(book_name.string!='韩国'and book_name.string!='男性'):
            book_name_clean=book_name.string
            print(num1, book_name_clean)

#menu_path_num = []

for item in soup.select('.d_menu&gt;ul&gt;li'):
    # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
    for a in item.find_all('a'):
        #print('a', a)
        # m 是 img标签中存在的属性
        menu_path = 'https://www.manhwa.cc/' + a.get('href')
        #count.append(menu_path)
        #menu_path_num.append(re.findall(r&quot;\d+\.?\d*&quot;, menu_path))
        menu_path_num=re.findall(r&quot;\d+\.?\d*&quot;, menu_path)

        #当前一部书爬取循环，从上面得到每一章地址后，遍历这么多“章”次

        #for num in menu_path_num:
        print('book_url:',menu_path)
        
        circle = requests.get(menu_path)
        # 将获取的图片地址依次放入count中
        count = []
        # 将获取的网页内容放入BeautifulSoup
        soup = BeautifulSoup(circle.text, 'lxml')

        for title in soup.select('div.fl.r_tab_l'):
            for title in title.find_all('span'):
                print('title:', title.text)
                title=title.text

        for item in soup.select('.r_img'):
            # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
            for img in item.find_all('img'):
                print('img_url:', img)
                # m 是 img标签中存在的属性
                img_path = img.get('data-original')
                count.append(img_path)
                
        # 用enumerate依次取出count中的图片地址 放入v中
        os.makedirs('D://manhua//整站漫画爬取//' +  book_name_clean + '//' + str(title) + '//')
        for i, v in enumerate(count):
            # 将获取的v值再次放入request中进行与网站相应
            image = requests.get(v)
            # 存取图片过程中，出现不能存储 int 类型，故而，我们对他进行类型转换 str()。w:读写方式打开，b：二进制进行读写。图片一般用到的都是二进制。
            with open('D://manhua//整站漫画爬取//' + book_name_clean + '//'+ str(title) + '//' +str(i) + '.jpg', 'wb') as file:
            #with open('C://Users//50159//Desktop//manhua//test//' + str(num1) + '_' + str(i) + '.jpg', 'wb') as file:
                # content：图片转换成二进制，进行保存。
                file.write(image.content)
            print(i)
						
</code></pre>
<hr>
<br>
<h2 id="爬取过程">爬取过程：</h2>
<hr>
<br>
<figure data-type="image" tabindex="3"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361188393.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="4"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361235369.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="5"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361244280.png" alt="" loading="lazy"></figure>
<p>基本可行，最高纪录 ，爬取四本后停止。</p>
<p>真的太多了，一本大小平均150M左右。</p>
<hr>
<br>
<h2 id="总结">总结：</h2>
<hr>
<br>
<p>爬取正本漫画 ✅</p>
<p>整站漫画半自动化爬取（停止需手动启动一次）✅</p>
<p>全自动下载网站漫画 （会被网站截停）❌</p>
<hr>
<h1 id="v20">v2.0</h1>
<br>
<center>
<p>2.0优化版</p>
</center>
<br>
<hr>
<br>
<h2 id="特点">特点</h2>
<br>
<p>ui界面添加✅</p>
<p>网站截停后 播放音乐提醒 接近半自动重启 ✅</p>
<p>各个细节爬取优化，优化接近自身无报错 ✅</p>
<p>cmd输出界面优化✅</p>
<p>计时器检测添加中（待）</p>
<p>全自动重启（待）</p>
<br>
<hr>
<br>
<h2 id="逻辑">逻辑</h2>
<br>
<figure data-type="image" tabindex="6"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565871675669.png" alt="" loading="lazy"></figure>
<br>
<hr>
<br>
<h2 id="代码">代码</h2>
<br>
<h2 id="启动代码">启动代码：</h2>
<pre><code>
import os

os.system(r&quot;python D:\manhua\这里是网址\manhua4.py&quot;)

os.system(r&quot;F:\CloudMusic\是萝莉控真是太好了.mp3&quot;)

</code></pre>
<h2 id="爬取代码">爬取代码：</h2>
<pre><code>
import requests
from bs4 import BeautifulSoup
import re
import os

from PyQt5.QtWidgets import QApplication, QWidget, QLineEdit, QInputDialog, QGridLayout, QLabel, QPushButton, QFrame, QProgressBar

first=1

class InputDialog(QWidget):

    def __init__(self):
        super(InputDialog,self).__init__()
        self.initUi()

    def initUi(self):
        self.setWindowTitle(&quot;漫画爬取&quot;)
        self.setGeometry(50,50,1200,600)

        label1=QLabel(&quot;第一本:&quot;)
        label2=QLabel(&quot;最后一本:&quot;)

        self.nameLable = QLabel(&quot;1&quot;)#1
        self.first=int(self.nameLable.text())
        self.nameLable.setText(str(self.first))
        self.nameLable.setFrameStyle(QFrame.Panel|QFrame.Sunken)
        self.styleLable = QLabel(&quot;1&quot;)#1030
        self.last=self.styleLable.text()
        self.styleLable.setText(str(self.last))
        self.styleLable.setFrameStyle(QFrame.Panel|QFrame.Sunken)

        # 设置进度条(弃用)

        nameButton=QPushButton(&quot;更改&quot;)
        nameButton.clicked.connect(self.selectName)
        styleButton=QPushButton(&quot;更改&quot;)
        styleButton.clicked.connect(self.selectStyle)
        okButton = QPushButton(&quot;OK&quot;)
        okButton.clicked.connect(self.selectOk)

        mainLayout=QGridLayout()
        mainLayout.addWidget(label1,0,0)
        mainLayout.addWidget(self.nameLable,0,1)
        mainLayout.addWidget(nameButton,0,2)
        mainLayout.addWidget(label2,1,0)
        mainLayout.addWidget(self.styleLable,1,1)
        mainLayout.addWidget(styleButton,1,2)
        mainLayout.addWidget(okButton,2,1)

        self.setLayout(mainLayout)

    #爬取代码
		
    def ManHua(self):

        for num1 in range(first,1030):
            import urllib.request  # url包

            def openUrl(circle):
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36',
                    'Host': 'jandan.net'
                }
                req = urllib.request.Request(circle, headers=headers)
                response = urllib.request.urlopen(req)  # 请求
                html = response.read()  # 获取
                html = html.decode(&quot;utf-8&quot;)  # 解码
                print(html)  # 打印

            if __name__ == &quot;__main__&quot;:
                circle = requests.get('https://这里是网址/book/' + str(num1))

            count = []
            soup = BeautifulSoup(circle.text, 'lxml')

            for item_book in soup.select('.d_bg_t'):
                for book_name in item_book.select('a')[0]:
                    book_name_clean = book_name.string
                    print('')
                    print(&quot;正在下载：&quot;,num1, book_name_clean)
                    aa=0
                    #print(aa,num1)
                    if num1&gt;aa:
                        aa=num1
                        #print(aa)
                        for i in range(int(num1*(100/1030))+1):
                            print('\r'+'总进度：' + '▇' * (i // 2) + str(i) + '%', end='')
                            print('')

            for item in soup.select('.d_menu&gt;ul&gt;li'):
                for a in item.find_all('a'):
                    menu_path = 'https://这里是网址/' + a.get('href')
                    # count.append(menu_path)
                    # menu_path_num.append(re.findall(r&quot;\d+\.?\d*&quot;, menu_path))
                    menu_path_num = re.findall(r&quot;\d+\.?\d*&quot;, menu_path)

                    # 当前一部书爬取循环，从上面得到每一章地址后，遍历这么多“章”次

                    # for num in menu_path_num:
                    #print('book_url:', menu_path)


                    circle = requests.get(menu_path)
                    count = []
                    soup = BeautifulSoup(circle.text, 'lxml')
                    #print(menu_path)
                    print('.', end='')

                    for title in soup.select('div.fl.r_tab_l'):
                        for title in title.find_all('span'):
                            #print('title:', title.text)
                            title = title.text

                    for item in soup.select('.r_img'):
                        for img in item.find_all('img'):
                            #print('img_url:', img)
                            img_path = img.get('data-original')
                            count.append(img_path)

                    #自动识别'文件夹+文件'重复后跳过下载如何continue
                    if(os.path.exists('D:/manhua/manhuatest/' + book_name_clean + '/' + str(title) + '/')):
                        continue
                    else:
                        os.makedirs('D:/manhua/manhuatest/' + book_name_clean + '/' + str(title) + '/')

                        for i, v in enumerate(count):
                            image = requests.get(v)
                            if (os.path.exists('D:/manhua/manhuatest/' + book_name_clean + '/' + str(title) + '/' + str(i) + '.jpg')):
                                continue
                            else:
                                with open('D:/manhua/manhuatest/' + book_name_clean + '/' + str(title) + '/' + str(i) + '.jpg', 'wb') as file:
                                    file.write(image.content)
                                #print(i)
                                continue
                        continue


    def selectName(self):
        name,ok = QInputDialog.getText(self,&quot;第一本&quot;,&quot;第一本序号:&quot;,
                                       QLineEdit.Normal,self.nameLable.text())
        if ok and (len(name)!=0):
            self.nameLable.setText(name)
    def selectStyle(self):
        style, ok = QInputDialog.getText(self, &quot;最后一本&quot;, &quot;最后一本序号:&quot;,
                                        QLineEdit.Normal, self.nameLable.text())
        if ok and (len(style)!=0):
            self.styleLable.setText(style)
    def selectOk(self):
        self.ManHua()

if __name__==&quot;__main__&quot;:
    import sys
    app=QApplication(sys.argv)
    myshow=InputDialog()
    myshow.show()
    sys.exit(app.exec_())


</code></pre>
<br>
<hr>
<br>
<h2 id="过程">过程</h2>
<br>
<figure data-type="image" tabindex="7"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565763309680.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="8"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565763331643.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="9"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565763348226.png" alt="" loading="lazy"></figure>
<br>
<hr>
<br>
<h2 id="总结-2">总结</h2>
<br>
<p>整站漫画全自动化爬取✅</p>
<p>不能自动重启❌</p>
<p>基本百分之95的功能实现，项目可宣布成功完成！✅</p>
<br>

                </div>
            </article>
        </div>

        
            <div class="next-post">
                <div class="next gt-c-content-color-first">下一篇</div>
                <a href="https://littlebuzi.github.io/tensorflow1" class="post-title gt-a-link">
                    Tensorflow
                </a>
            </div>
        

        

        
            
                <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'e7c2b33da7e627d57c5a',
    clientSecret: '70bd30bae9adc0e0559d863df193af430483bcd1',
    repo: 'littlebuzi.github.io',
    owner: 'littlebuzi',
    admin: ['littlebuzi'],
    id: location.pathname,      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

            

            
        

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">记录平凡的修炼之路</div>
    <div class="social-container">
        
            
                <a href="https://github.com/littlebuzi/" target="_blank">
                    <i class="fab fa-github gt-c-content-color-first"></i>
                </a>
            
        
            
        
            
        
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
    <div>
        Theme by <a href="https://imhanjie.com/" target="_blank">imhanjie</a>, Powered by <a
                href="https://github.com/getgridea/gridea" target="_blank">Gridea | <a href="https://littlebuzi.github.io//atom.xml" target="_blank">RSS</a></a>
    </div>
</div>

<script>
    hljs.initHighlightingOnLoad()
</script>

    </div>
</div>
</body>
</html>
