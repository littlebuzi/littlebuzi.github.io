<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>制作“漫画网站”爬虫 1.0 | buzi</title>
<meta name="description" content="德丽莎世界第一可爱">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://littlebuzi.github.io//favicon.ico?v=1573551318165">
<link rel="stylesheet" href="https://littlebuzi.github.io//styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>

<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />



  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://littlebuzi.github.io/">
        <img src="https://littlebuzi.github.io//images/avatar.png?v=1573551318165" class="site-logo">
        <h1 class="site-title">buzi</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            时间线
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            分类
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
          <a class="social-link" href="https://github.com/littlebuzi" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      德丽莎世界第一可爱
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">制作“漫画网站”爬虫 1.0</h2>
            <div class="post-date">2019-08-09</div>
            
              <div class="feature-container" style="background-image: url('https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1572349483&amp;di=f04068ac2afd19fb62abd1cf234b76d0&amp;imgtype=jpg&amp;er=1&amp;src=http%3A%2F%2Fwx2.sinaimg.cn%2Flarge%2F006qkgxGgy1g2ux90gsgvj30ff08odks.jpg')">
              </div>
            
            <div class="post-content">
              <hr>
<br>
<h1 id="目的">目的</h1>
<br>
<p>1.目标网站：https://www.这里是网址.cc/</p>
<figure data-type="image" tabindex="1"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565345268608.png" alt=""></figure>
<p>2.目标结果：获取全部漫画图片文件，并分好文件夹</p>
<hr>
<br>
<h1 id="实现过程">实现过程</h1>
<br>
<h2 id="基本逻辑">基本逻辑</h2>
<figure data-type="image" tabindex="2"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565360279264.png" alt=""></figure>
<h2 id="代码实现">代码实现</h2>
<br>
<pre><code>
import requests
from bs4 import BeautifulSoup
import re
import os

#1-1030
for num1 in range(1,1031):
    circle = requests.get('https://这里是网址/book/'+str(num1))
    # 将获取的图片地址依次放入count中
    count = []
    # 将获取的网页内容放入BeautifulSoup
    soup = BeautifulSoup(circle.text, 'lxml')
    # 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.find_all('a'):
        if(book_name.string!='韩国'and book_name.string!='男性'):
            book_name_clean=book_name.string
            print(num1, book_name_clean)

os.makedirs('D://manhua//整站漫画爬取//' + str(num1) +'.'+ book_name_clean )

#menu_path_num = []

for item in soup.select('.d_menu&gt;ul&gt;li'):
    # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
    for a in item.find_all('a'):
        #print('a', a)
        # m 是 img标签中存在的属性
        menu_path = 'https://www.manhwa.cc/' + a.get('href')
        #count.append(menu_path)
        #menu_path_num.append(re.findall(r&quot;\d+\.?\d*&quot;, menu_path))
        menu_path_num=re.findall(r&quot;\d+\.?\d*&quot;, menu_path)

        #当前一部书爬取循环，从上面得到每一章地址后，遍历这么多“章”次

        #for num in menu_path_num:
        print('book_url:',menu_path)
        circle = requests.get(menu_path)
        # 将获取的图片地址依次放入count中
        count = []
        # 将获取的网页内容放入BeautifulSoup
        soup = BeautifulSoup(circle.text, 'lxml')
        # 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list
				
        for title in soup.select('div.fl.r_tab_l'):
            for title in title.find_all('span'):
                print('title:', title.text)
                title=title.text

        for item in soup.select('.r_img'):
            # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
            for img in item.find_all('img'):
                print('img_url:', img)
                # m 是 img标签中存在的属性
                img_path = img.get('data-original')
                count.append(img_path)
        # 用enumerate依次取出count中的图片地址 放入v中
        os.makedirs('D://manhua//整站漫画爬取//' +  book_name_clean + '//' + str(title) + '//')
        for i, v in enumerate(count):
            # 将获取的v值再次放入request中进行与网站相应
            image = requests.get(v)
            # 存取图片过程中，出现不能存储 int 类型，故而，我们对他进行类型转换 str()。w:读写方式打开，b：二进制进行读写。图片一般用到的都是二进制。
            with open('D://manhua//整站漫画爬取//' + book_name_clean + '//'+ str(title) + '//' +str(i) + '.jpg', 'wb') as file:
            #with open('C://Users//50159//Desktop//manhua//test//' + str(num1) + '_' + str(i) + '.jpg', 'wb') as file:
                # content：图片转换成二进制，进行保存。
                file.write(image.content)
            print(i)
</code></pre>
<p>到这基本工作已完成，进入测试阶段，出现以下</p>
<hr>
<br>
<h1 id="测试问题">测试问题</h1>
<p>1.第250本左右，书名字开始出现异常，爬取书名有其他文字并出现混乱，因为之前是通过最前面几本书的情况，通过抛弃字样，来筛选出书名，而后1030本里标签发生变动，所以之后通过只取第一个出现的标签代替现在的筛选。</p>
<p>2.文件夹命名及生成文件夹出错，由于整理时出现混乱，代码写重复了。而后修改。</p>
<p>3.中途停止，可能是网站识别到了这是爬虫，而后添加伪浏览器头部head，还是会停，基本是connect超时。</p>
<p>针对上面问题，修改成了2.0版本:</p>
<pre><code>
import requests
from bs4 import BeautifulSoup
import re
import os

#1-1030
for num1 in range(2,1031):
    #716字符问题无法生成文件夹
import urllib.request  # url包

def openUrl(circle):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36',
        'Host': 'jandan.net'
    }
    req = urllib.request.Request(circle, headers=headers)
    response = urllib.request.urlopen(req)  # 请求
    html = response.read()  # 获取
    html = html.decode(&quot;utf-8&quot;)  # 解码
    print(html)  # 打印

if __name__ == &quot;__main__&quot;:
    circle = requests.get('https://这里是网址/book/' + str(num1))

# 将获取的图片地址依次放入count中
count = []
# 将获取的网页内容放入BeautifulSoup
soup = BeautifulSoup(circle.text, 'lxml')
# 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.select('a')[0]:
        book_name_clean = book_name.string
        print(num1, book_name_clean)

#os.makedirs('D://manhua//整站漫画爬取//' + str(num1) +'.'+ book_name_clean )

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.find_all('a'):
        if(book_name.string!='韩国'and book_name.string!='男性'):
            book_name_clean=book_name.string
            print(num1, book_name_clean)

#menu_path_num = []

for item in soup.select('.d_menu&gt;ul&gt;li'):
    # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
    for a in item.find_all('a'):
        #print('a', a)
        # m 是 img标签中存在的属性
        menu_path = 'https://www.manhwa.cc/' + a.get('href')
        #count.append(menu_path)
        #menu_path_num.append(re.findall(r&quot;\d+\.?\d*&quot;, menu_path))
        menu_path_num=re.findall(r&quot;\d+\.?\d*&quot;, menu_path)

        #当前一部书爬取循环，从上面得到每一章地址后，遍历这么多“章”次

        #for num in menu_path_num:
        print('book_url:',menu_path)
        
        circle = requests.get(menu_path)
        # 将获取的图片地址依次放入count中
        count = []
        # 将获取的网页内容放入BeautifulSoup
        soup = BeautifulSoup(circle.text, 'lxml')

        for title in soup.select('div.fl.r_tab_l'):
            for title in title.find_all('span'):
                print('title:', title.text)
                title=title.text

        for item in soup.select('.r_img'):
            # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
            for img in item.find_all('img'):
                print('img_url:', img)
                # m 是 img标签中存在的属性
                img_path = img.get('data-original')
                count.append(img_path)
                
        # 用enumerate依次取出count中的图片地址 放入v中
        os.makedirs('D://manhua//整站漫画爬取//' +  book_name_clean + '//' + str(title) + '//')
        for i, v in enumerate(count):
            # 将获取的v值再次放入request中进行与网站相应
            image = requests.get(v)
            # 存取图片过程中，出现不能存储 int 类型，故而，我们对他进行类型转换 str()。w:读写方式打开，b：二进制进行读写。图片一般用到的都是二进制。
            with open('D://manhua//整站漫画爬取//' + book_name_clean + '//'+ str(title) + '//' +str(i) + '.jpg', 'wb') as file:
            #with open('C://Users//50159//Desktop//manhua//test//' + str(num1) + '_' + str(i) + '.jpg', 'wb') as file:
                # content：图片转换成二进制，进行保存。
                file.write(image.content)
            print(i)
						
</code></pre>
<hr>
<br>
<h1 id="爬取过程">爬取过程：</h1>
<hr>
<br>
<figure data-type="image" tabindex="3"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361188393.png" alt=""></figure>
<figure data-type="image" tabindex="4"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361235369.png" alt=""></figure>
<figure data-type="image" tabindex="5"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361244280.png" alt=""></figure>
<p>基本可行，最高纪录 ，爬取四本后停止。</p>
<p>真的太多了，一本大小平均150M左右。</p>
<hr>
<br>
<h1 id="总结">总结：</h1>
<hr>
<br>
<p>爬取正本漫画 ✅</p>
<p>整站漫画半自动化爬取（停止需手动启动一次）✅</p>
<p>全自动下载网站漫画 （会被网站截停）❌</p>

            </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://littlebuzi.github.io//post/diguo">
                  <h3 class="post-title">
                    帝国时代Ⅱ—2018年世界高手排名前10
                  </h3>
                </a>
              </div>
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>

<script type="application/javascript">

AOS.init();

hljs.initHighlightingOnLoad()

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>


  
  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: 'e7c2b33da7e627d57c5a',
        clientSecret: '70bd30bae9adc0e0559d863df193af430483bcd1',
        repo: 'littlebuzi.github.io',
        owner: 'littlebuzi',
        admin: ['littlebuzi'],
        id: location.pathname,      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
