
<html>
  <head lang="zh">
        <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"/>
        <meta content="yes" name="apple-mobile-web-app-capable"/>
        <meta content="black" name="apple-mobile-web-app-status-bar-style"/>
        <meta content="telephone=no" name="format-detection"/>
        <meta name="renderer" content="webkit">
    <title>制作“漫画网站”爬虫 1.0 | buzi</title>
<link href="https://littlebuzi.github.io//styles/main.css" type="text/css" rel="stylesheet"/>
<script type="text/javascript" src="https://littlebuzi.github.io//media/scripts/jquery.js"></script>
<script type="text/javascript" src="https://littlebuzi.github.io//media/scripts/basic.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

  </head>

  <body>
     <div class="header">
      <div class="logo_title">
		  
        <div class="title animated fadeInDown"><img src="https://littlebuzi.github.io//images/avatar.png?v=1575879601078"/>

          <h1 title="buzi" class="weaklink"><a href="/">buzi</a>

          </h1>

          <div class="navbar weaklink">
            <div class="normal_nav">

<div class="bitcron_nav_container">


  <div class="bitcron_nav">
    <div class="mixed_site_nav_wrap site_nav_wrap">
		
      <ul class="mixed_site_nav site_nav sm sm-base">
 
  <li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/" class="selected active current nav__item" >首页</a>

  </li>
 
  <li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/archives" class="selected active current nav__item" >时间线</a>

  </li>
 
  <li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/tags" class="selected active current nav__item" >分类</a>

  </li>
 
  <li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/post/about" class="selected active current nav__item" >关于</a>

  </li>
 

      </ul>

      <div class="clear clear_nav_inline_end"></div>

    </div>

  </div>



  <div class="clear clear_nav_end"></div>

</div>

            </div>

            <div class="hamberger"><i class="fa fa-bars"></i>
<i class="fa fa-times"></i>

            </div>

          </div>

        </div>

      </div>

      <div class="hidden_nav animated fadeInDown">

<div class="bitcron_nav_container">


  <div class="bitcron_nav">
    <div class="mixed_site_nav_wrap site_nav_wrap">
      <ul class="mixed_site_nav site_nav sm sm-base">
		  
	
  <li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/" class="selected active current nav__item" >首页</a>

  </li>


  <li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/archives" class="selected active current nav__item" >时间线</a>

  </li>


  <li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/tags" class="selected active current nav__item" >分类</a>

  </li>


  <li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/post/about" class="selected active current nav__item" >关于</a>

  </li>





      </ul>

      <div class="clear clear_nav_inline_end"></div>

    </div>

  </div>



  <div class="clear clear_nav_end"></div>

</div>

      </div>

    </div>


    <div class="main">
      <div class="main-inner">


<div class="content">






  <div class="post_page" >

<div class="post">
  <div class="post_title sm_margin">
    <h2><a>制作“漫画网站”爬虫 1.0</a>



    </h2>
  </div>

  <div class="post_details">
    <div class="info"><i class="fa fa-clock-o"></i>
<span class="date_info">2019-08-09</span>
<i class="fa fa-eye"></i>

<span class="date_info"><span id="busuanzi_value_page_pv"></span> Views</span>


<i class="fa fa-bookmark-o"></i>
<span class="tags_info weaklink">
	

</span>


    </div>

  </div>





  <div class="post_content markdown"><p class="md_block">
    <span class="md_line md_line_start md_line_end"><hr>
<br>
<h1 id="目的">目的</h1>
<br>
<p>1.目标网站：https://www.这里是网址.cc/</p>
<figure data-type="image" tabindex="1"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565345268608.png" alt=""></figure>
<p>2.目标结果：获取全部漫画图片文件，并分好文件夹</p>
<hr>
<br>
<h1 id="实现过程">实现过程</h1>
<br>
<h2 id="基本逻辑">基本逻辑</h2>
<figure data-type="image" tabindex="2"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565360279264.png" alt=""></figure>
<h2 id="代码实现">代码实现</h2>
<br>
<pre><code>
import requests
from bs4 import BeautifulSoup
import re
import os

#1-1030
for num1 in range(1,1031):
    circle = requests.get('https://这里是网址/book/'+str(num1))
    # 将获取的图片地址依次放入count中
    count = []
    # 将获取的网页内容放入BeautifulSoup
    soup = BeautifulSoup(circle.text, 'lxml')
    # 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.find_all('a'):
        if(book_name.string!='韩国'and book_name.string!='男性'):
            book_name_clean=book_name.string
            print(num1, book_name_clean)

os.makedirs('D://manhua//整站漫画爬取//' + str(num1) +'.'+ book_name_clean )

#menu_path_num = []

for item in soup.select('.d_menu&gt;ul&gt;li'):
    # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
    for a in item.find_all('a'):
        #print('a', a)
        # m 是 img标签中存在的属性
        menu_path = 'https://www.manhwa.cc/' + a.get('href')
        #count.append(menu_path)
        #menu_path_num.append(re.findall(r&quot;\d+\.?\d*&quot;, menu_path))
        menu_path_num=re.findall(r&quot;\d+\.?\d*&quot;, menu_path)

        #当前一部书爬取循环，从上面得到每一章地址后，遍历这么多“章”次

        #for num in menu_path_num:
        print('book_url:',menu_path)
        circle = requests.get(menu_path)
        # 将获取的图片地址依次放入count中
        count = []
        # 将获取的网页内容放入BeautifulSoup
        soup = BeautifulSoup(circle.text, 'lxml')
        # 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list
				
        for title in soup.select('div.fl.r_tab_l'):
            for title in title.find_all('span'):
                print('title:', title.text)
                title=title.text

        for item in soup.select('.r_img'):
            # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
            for img in item.find_all('img'):
                print('img_url:', img)
                # m 是 img标签中存在的属性
                img_path = img.get('data-original')
                count.append(img_path)
        # 用enumerate依次取出count中的图片地址 放入v中
        os.makedirs('D://manhua//整站漫画爬取//' +  book_name_clean + '//' + str(title) + '//')
        for i, v in enumerate(count):
            # 将获取的v值再次放入request中进行与网站相应
            image = requests.get(v)
            # 存取图片过程中，出现不能存储 int 类型，故而，我们对他进行类型转换 str()。w:读写方式打开，b：二进制进行读写。图片一般用到的都是二进制。
            with open('D://manhua//整站漫画爬取//' + book_name_clean + '//'+ str(title) + '//' +str(i) + '.jpg', 'wb') as file:
            #with open('C://Users//50159//Desktop//manhua//test//' + str(num1) + '_' + str(i) + '.jpg', 'wb') as file:
                # content：图片转换成二进制，进行保存。
                file.write(image.content)
            print(i)
</code></pre>
<p>到这基本工作已完成，进入测试阶段，出现以下</p>
<hr>
<br>
<h1 id="测试问题">测试问题</h1>
<p>1.第250本左右，书名字开始出现异常，爬取书名有其他文字并出现混乱，因为之前是通过最前面几本书的情况，通过抛弃字样，来筛选出书名，而后1030本里标签发生变动，所以之后通过只取第一个出现的标签代替现在的筛选。</p>
<p>2.文件夹命名及生成文件夹出错，由于整理时出现混乱，代码写重复了。而后修改。</p>
<p>3.中途停止，可能是网站识别到了这是爬虫，而后添加伪浏览器头部head，还是会停，基本是connect超时。</p>
<p>针对上面问题，修改成了2.0版本:</p>
<pre><code>
import requests
from bs4 import BeautifulSoup
import re
import os

#1-1030
for num1 in range(2,1031):
    #716字符问题无法生成文件夹
import urllib.request  # url包

def openUrl(circle):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36',
        'Host': 'jandan.net'
    }
    req = urllib.request.Request(circle, headers=headers)
    response = urllib.request.urlopen(req)  # 请求
    html = response.read()  # 获取
    html = html.decode(&quot;utf-8&quot;)  # 解码
    print(html)  # 打印

if __name__ == &quot;__main__&quot;:
    circle = requests.get('https://这里是网址/book/' + str(num1))

# 将获取的图片地址依次放入count中
count = []
# 将获取的网页内容放入BeautifulSoup
soup = BeautifulSoup(circle.text, 'lxml')
# 根据谷歌SelectGadGet这个插件，获取html标签，比如获取：#gallery-list

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.select('a')[0]:
        book_name_clean = book_name.string
        print(num1, book_name_clean)

#os.makedirs('D://manhua//整站漫画爬取//' + str(num1) +'.'+ book_name_clean )

for item_book in soup.select('.d_bg_t'):
    for book_name in item_book.find_all('a'):
        if(book_name.string!='韩国'and book_name.string!='男性'):
            book_name_clean=book_name.string
            print(num1, book_name_clean)

#menu_path_num = []

for item in soup.select('.d_menu&gt;ul&gt;li'):
    # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
    for a in item.find_all('a'):
        #print('a', a)
        # m 是 img标签中存在的属性
        menu_path = 'https://www.manhwa.cc/' + a.get('href')
        #count.append(menu_path)
        #menu_path_num.append(re.findall(r&quot;\d+\.?\d*&quot;, menu_path))
        menu_path_num=re.findall(r&quot;\d+\.?\d*&quot;, menu_path)

        #当前一部书爬取循环，从上面得到每一章地址后，遍历这么多“章”次

        #for num in menu_path_num:
        print('book_url:',menu_path)
        
        circle = requests.get(menu_path)
        # 将获取的图片地址依次放入count中
        count = []
        # 将获取的网页内容放入BeautifulSoup
        soup = BeautifulSoup(circle.text, 'lxml')

        for title in soup.select('div.fl.r_tab_l'):
            for title in title.find_all('span'):
                print('title:', title.text)
                title=title.text

        for item in soup.select('.r_img'):
            # 用bs4中的find_all获取 #gallery-list 中是否存在 img这个标签
            for img in item.find_all('img'):
                print('img_url:', img)
                # m 是 img标签中存在的属性
                img_path = img.get('data-original')
                count.append(img_path)
                
        # 用enumerate依次取出count中的图片地址 放入v中
        os.makedirs('D://manhua//整站漫画爬取//' +  book_name_clean + '//' + str(title) + '//')
        for i, v in enumerate(count):
            # 将获取的v值再次放入request中进行与网站相应
            image = requests.get(v)
            # 存取图片过程中，出现不能存储 int 类型，故而，我们对他进行类型转换 str()。w:读写方式打开，b：二进制进行读写。图片一般用到的都是二进制。
            with open('D://manhua//整站漫画爬取//' + book_name_clean + '//'+ str(title) + '//' +str(i) + '.jpg', 'wb') as file:
            #with open('C://Users//50159//Desktop//manhua//test//' + str(num1) + '_' + str(i) + '.jpg', 'wb') as file:
                # content：图片转换成二进制，进行保存。
                file.write(image.content)
            print(i)
						
</code></pre>
<hr>
<br>
<h1 id="爬取过程">爬取过程：</h1>
<hr>
<br>
<figure data-type="image" tabindex="3"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361188393.png" alt=""></figure>
<figure data-type="image" tabindex="4"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361235369.png" alt=""></figure>
<figure data-type="image" tabindex="5"><img src="https://littlebuzi.github.io//post-images/2019/08/09/python/manhua/1565361244280.png" alt=""></figure>
<p>基本可行，最高纪录 ，爬取四本后停止。</p>
<p>真的太多了，一本大小平均150M左右。</p>
<hr>
<br>
<h1 id="总结">总结：</h1>
<hr>
<br>
<p>爬取正本漫画 ✅</p>
<p>整站漫画半自动化爬取（停止需手动启动一次）✅</p>
<p>全自动下载网站漫画 （会被网站截停）❌</p>
</p>

     <p class="md_block">
    <div class="reward"><div class="reward-button">赏 <span class="reward-code"> <span class="alipay-code"> <img class="alipay-img" src="https://littlebuzi.github.io//media/images/alipay.png"><b>支付宝扫码打赏</b> </span> <span class="wechat-code"> <img class="wechat-img" src="https://littlebuzi.github.io//media/images/wechat.png"><b>微信打赏</b> </span> </span></div></div>
</p> 
</div>

</div>



<link href="https://littlebuzi.github.io//styles/main.css" type="text/css" rel="stylesheet"/>

<div class="doc_comments">

          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'e7c2b33da7e627d57c5a',
    clientSecret: '70bd30bae9adc0e0559d863df193af430483bcd1',
    repo: 'littlebuzi.github.io',
    owner: 'littlebuzi',
    admin: ['littlebuzi'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          
			  
          
        
</div>



  </div>
</div>



      </div>




    </div>

   <div class="footer">
<link href="https://littlebuzi.github.io//styles/main.css" type="text/css" rel="stylesheet"/><div class="site_footer_wrap"><div class="site_footer">

      <div class="mysocials"><div class="my_socials">
		   
			   
    
			   
    
			   
    
			   
    
</div><link href="https://littlebuzi.github.io//styles/main.css" type="text/css" rel="stylesheet"/>

      </div>

      <div class="copyright">Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
      </div>

</div></div>

    </div>


<style type="text/css">a.back_to_top {
    text-decoration: none;
    position: fixed;
    bottom: 40px;
    right: 30px;
    background: #f0f0f0;
    height: 40px;
    width: 40px;
    border-radius: 50%;
    line-height: 36px;
    font-size: 18px;
    text-align: center;
    transition-duration: .5s;
    transition-propety: background-color;
    display: none;
}

a.back_to_top span {
    color: #888;
}

a.back_to_top:hover {
    cursor: pointer;
    background: #dfdfdf;
}

a.back_to_top:hover span {
    color: #555;
}

@media print, screen and (max-width: 580px) {
    .back_to_top {
        display: none !important;
    }
}



</style><a id="back_to_top" href="#" class="back_to_top"><span>△</span>
</a>
<script type="text/javascript" src="https://littlebuzi.github.io//media/scripts/jquery.js"></script>

<script>$(document).ready((function(_this) {
  return function() {
    var bt;
    bt = $('#back_to_top');
    if ($(document).width() > 480) {
      $(window).scroll(function() {
        var st;
        st = $(window).scrollTop();
        if (st > 30) {
          return bt.css('display', 'block');
        } else {
          return bt.css('display', 'none');
        }
      });
      return bt.click(function() {
        $('body,html').animate({
          scrollTop: 0
        }, 800);
        return false;
      });
    }
  };
})(this));
</script>

</body>

</html>